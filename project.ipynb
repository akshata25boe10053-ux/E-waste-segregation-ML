{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFhwXCCUQ41+HZd2DTPx81"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOTYYeR5NgmY","executionInfo":{"status":"ok","timestamp":1764010407086,"user_tz":-330,"elapsed":29,"user":{"displayName":"AKSHATA N VENUGOPAL 25BOE10053","userId":"00900900480862591897"}},"outputId":"85439941-44c5-4a3a-b6e8-5d344e2cb883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Project created successfully!\n"]}],"source":["import os\n","\n","project = {\n","    \"E-Waste-Segregation/src/data_loader.py\": \"\"\"from pathlib import Path\n","from skimage.io import imread\n","from skimage.transform import resize\n","import numpy as np\n","\n","def load_dataset(data_dir, classes=None, images_per_class=None, image_size=(128,128), as_gray=True):\n","    data_dir = Path(data_dir)\n","    if classes is None:\n","        classes = sorted([p.name for p in data_dir.iterdir() if p.is_dir()])\n","    X_list, y_list, class_names = [], [], []\n","    for idx, cls in enumerate(classes):\n","        cls_dir = data_dir / cls\n","        imgs = sorted([p for p in cls_dir.iterdir() if p.is_file()])\n","        if images_per_class:\n","            imgs = imgs[:images_per_class]\n","        for img_path in imgs:\n","            try:\n","                img = imread(str(img_path), as_gray=as_gray)\n","                img = resize(img, image_size, anti_aliasing=True)\n","                X_list.append(img)\n","                y_list.append(idx)\n","            except:\n","                continue\n","        class_names.append(cls)\n","    return np.stack(X_list, axis=0), np.array(y_list), class_names\n","\"\"\",\n","\n","    \"E-Waste-Segregation/src/preprocessing.py\": \"\"\"import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import joblib\n","\n","def flatten_images(X):\n","    return X.reshape(X.shape[0], -1)\n","\n","def scale_features(X_train, X_val=None, scaler_path=None):\n","    scaler = StandardScaler()\n","    X_train_s = scaler.fit_transform(X_train)\n","    X_val_s = scaler.transform(X_val) if X_val is not None else None\n","    if scaler_path: joblib.dump(scaler, scaler_path)\n","    return scaler, X_train_s, X_val_s\n","\"\"\",\n","\n","    \"E-Waste-Segregation/src/model_train.py\": \"\"\"import joblib\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","def train_random_forest(X, y, model_path, n_estimators=200, test_size=0.2):\n","    X = np.asarray(X); y = np.asarray(y)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\n","    clf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1)\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    joblib.dump(clf, model_path)\n","    return {\n","        \"accuracy\": accuracy_score(y_test, y_pred),\n","        \"report\": classification_report(y_test, y_pred, output_dict=True),\n","        \"confusion\": confusion_matrix(y_test, y_pred)\n","    }\n","\"\"\",\n","\n","    \"E-Waste-Segregation/src/model_evaluate.py\": \"\"\"import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion_matrix(cm, class_names):\n","    sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n","    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.show()\n","\n","def print_classification_report(report_dict):\n","    import json; print(json.dumps(report_dict, indent=2))\n","\"\"\",\n","\n","    \"E-Waste-Segregation/src/predict.py\": \"\"\"import joblib\n","from skimage.io import imread\n","from skimage.transform import resize\n","\n","def load_model(p): return joblib.load(p)\n","\n","def preprocess_single_image(img_path, image_size=(128,128), as_gray=True):\n","    img = imread(img_path, as_gray=as_gray)\n","    img = resize(img, image_size, anti_aliasing=True)\n","    return img.reshape(1, -1)\n","\n","def predict_single(model, img_path, scaler=None, class_names=None):\n","    x = preprocess_single_image(img_path)\n","    if scaler: x = scaler.transform(x)\n","    pred = model.predict(x)[0]\n","    return class_names[pred] if class_names else int(pred)\n","\"\"\",\n","\n","    \"E-Waste-Segregation/src/utils.py\": \"\"\"import os\n","def ensure_dir(path): os.makedirs(path, exist_ok=True)\n","def list_class_folders(data_dir): return sorted(os.listdir(data_dir))\n","\"\"\",\n","\n","    \"E-Waste-Segregation/src/colab_helper.py\": \"\"\"from google.colab import drive\n","from pathlib import Path\n","\n","def mount_drive(): drive.mount('/content/drive')\n","def get_data_dir(path):\n","    p = Path(path)\n","    if not p.exists(): raise FileNotFoundError(f\"{p} not found\")\n","    return str(p)\n","\"\"\",\n","\n","    \"E-Waste-Segregation/notebooks/colab_notebook_cells.md\": \"\"\"# Colab notebook cells\n","\n","### 1) Mount drive\n","from src.colab_helper import mount_drive, get_data_dir\n","mount_drive()\n","DATA_DIR = get_data_dir('/content/drive/MyDrive/E_waste_dataset')\n","\n","### 2) Load images\n","from src.data_loader import load_dataset\n","X, y, class_names = load_dataset(DATA_DIR, image_size=(128,128), as_gray=True)\n","\n","### 3) Preprocess\n","from src.preprocessing import flatten_images, scale_features\n","X_flat = flatten_images(X)\n","scaler, X_scaled, _ = scale_features(X_flat)\n","\n","### 4) Train model\n","from src.model_train import train_random_forest\n","res = train_random_forest(X_scaled, y, '/content/drive/MyDrive/e_waste_model.joblib')\n","\n","### 5) Evaluate\n","from src.model_evaluate import plot_confusion_matrix, print_classification_report\n","print_classification_report(res[\"report\"])\n","plot_confusion_matrix(res[\"confusion\"], class_names)\n","\"\"\",\n","\n","    \"E-Waste-Segregation/requirements.txt\": \"\"\"scikit-learn\n","scikit-image\n","joblib\n","matplotlib\n","seaborn\n","numpy\n","\"\"\",\n","\n","    \"E-Waste-Segregation/README.md\": \"\"\"# E-Waste Segregation using Random Forest\n","\n","A complete machine-learning pipeline to classify images of electronic waste into categories:\n","- mobile\n","- battery\n","- charger\n","- PCB\n","- laptop\n","\n","Includes:\n","- data loading\n","- preprocessing\n","- model training\n","- evaluation\n","- prediction scripts\n","\"\"\",\n","\n","    \"E-Waste-Segregation/statement.md\": \"\"\"# Project Statement\n","\n","This project classifies e-waste images using a Random Forest trained on flattened image pixels.\n","\"\"\",\n","}\n","\n","# --- Write files ---\n","for path, content in project.items():\n","    os.makedirs(os.path.dirname(path), exist_ok=True)\n","    with open(path, \"w\") as f:\n","        f.write(content)\n","\n","print(\"Project created successfully!\")\n"]}]}